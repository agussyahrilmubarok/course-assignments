# Min In Sync Replica

## Introduction

Apache Kafka is a **distributed event streaming platform** designed for high throughput, fault tolerance, and scalability.
It is commonly used for:

* **Event-driven architectures** (e.g., order events, user activity tracking).
* **Real-time data pipelines** (streaming from applications to databases, analytics systems, or other services).
* **Decoupling microservices** through asynchronous communication.

Kafka organizes data into **topics**, which are further divided into **partitions** and replicated across brokers. Replication ensures both parallelism and fault tolerance.

In this article, we will focus on the concept of **Minimum In-Sync Replicas (min.insync.replicas)** and why it is critical for durability and reliability in a Kafka cluster.

---

## What Is Min In-Sync Replicas?

A Kafka topic partition always has one **leader replica** and several **follower replicas**. Followers copy data from the leader to maintain redundancy. The set of replicas that are fully caught up with the leader is known as the **In-Sync Replica set (ISR)**.

The configuration `min.insync.replicas` specifies the **minimum number of replicas** (including the leader) that must acknowledge a write before it is considered successful.

If the number of available replicas falls below this threshold, Kafka will reject produce requests with an error (`NotEnoughReplicasException`).

---

## Why Is It Important?

1. **Durability**
   If `min.insync.replicas` is set too low (e.g., 1), a producer can succeed even if only the leader has written the message. If the leader crashes before followers catch up, that data can be lost.

2. **Trade-off Between Availability and Safety**

   * Lower values (e.g., 1) → higher availability, lower durability.
   * Higher values (e.g., 2 or more) → stronger durability guarantees, but may reduce availability if some brokers are offline.

3. **Recommended Setting**
   In a 3-broker cluster with `replication.factor=3`, it is common to set:

   * `min.insync.replicas=2`
     This ensures at least the leader plus one follower confirm the write, balancing safety and availability.

---

## Example Configurations

### Broker-Level Default

Add the following environment variable to each Kafka broker (e.g., in `docker-compose.yml`):

```yaml
environment:
  KAFKA_MIN_INSYNC_REPLICAS: 2
```

This sets the default for all newly created topics, unless overridden.

---

### Topic-Level Override

You can also set or update `min.insync.replicas` for a specific topic:

```bash
# Create topic with custom min.insync.replicas
kafka-topics --bootstrap-server kafka1:29092 \
  --create \
  --topic example.order.events \
  --partitions 3 \
  --replication-factor 3 \
  --config min.insync.replicas=2

# Alter existing topic
kafka-configs --bootstrap-server kafka1:29092 \
  --alter --entity-type topics --entity-name example.order.events \
  --add-config min.insync.replicas=2
```

---

## Spring Boot Application

### Dependency

Add the following dependency in your `pom.xml`:

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

---

### Producer Server

#### Environment Order Configuration (`application.yml`)

```yml
spring:
  kafka:
    bootstrap-servers: localhost:9092, localhost:9094, localhost:9096
    producer:
      # Kafka Producer configs
      acks: all
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    # Custom Topic Configs
    topics:
      order-events:
        name: "example.order.events"
        partitions: 3
        replicas: 3
        min-insync-replicas: 2
```

#### Kafka Configuration

```java
@Configuration
public class KafkaConfig {

    @Value("${spring.kafka.topics.order-events.name}")
    private String orderEventsTopicName;

    @Value("${spring.kafka.topics.order-events.partitions}")
    private int orderEventsPartitions;

    @Value("${spring.kafka.topics.order-events.replicas}")
    private short orderEventsReplicas;

    @Value("${spring.kafka.topics.order-events.min-insync-replicas}")
    private String orderEventsMinInSyncReplicas;

    @Bean
    public NewTopic orderEventsTopic() {
        return TopicBuilder.name(orderEventsTopicName)
                .partitions(orderEventsPartitions)
                .replicas(orderEventsReplicas)
                .config("min.insync.replicas", orderEventsMinInSyncReplicas)
                .build();
    }
}
```

#### Kafka Producer Message Model

```java
@Getter
@Setter
@Builder
public class OrderEvent {
    private UUID eventId;              // unique per event (important for idempotency)
    private OrderEventType eventType;  // CREATED, CANCELLED
    private OffsetDateTime eventAt;    // when the event was published
    private OrderPayload order;        // order details related to this event

    public enum OrderEventType {
        CREATED,
        PAID,
        PROCESSING,
        SHIPPED,
        DELIVERED,
        CANCELLED
    }

    @Getter
    @Builder
    @NoArgsConstructor(access = AccessLevel.PRIVATE)
    @AllArgsConstructor(access = AccessLevel.PRIVATE)
    public static class OrderPayload {
        private String orderId;
        private String customerId;
        private BigDecimal totalAmount;
        private OffsetDateTime orderAt;

        public static OrderPayload from(Order order) {
            return OrderPayload.builder()
                    .orderId(order.getId().toString())
                    .customerId(order.getCustomerId())
                    .totalAmount(order.getTotalAmount())
                    .orderAt(order.getOrderAt())
                    .build();
        }
    }
}
```

#### Kafka Producer

```java
@Component
@Slf4j
public class OrderEventProducer {

    @Value("${spring.kafka.topics.order-events.name}")
    private String orderEventsTopicName;

    private final KafkaTemplate<String, String> kafkaTemplate;
    private final ObjectMapper objectMapper;

    public OrderEventProducer(KafkaTemplate<String, String> kafkaTemplate, ObjectMapper objectMapper) {
        this.kafkaTemplate = kafkaTemplate;
        this.objectMapper = objectMapper;
    }

    /**
     * Serialize OrderEvent and send it asynchronously to Kafka.
     */
    public CompletableFuture<SendResult<String, String>> sendEvent(OrderEvent event) throws JsonProcessingException {
        String key = event.getOrder().getOrderId(); // partition key based on orderId
        String value = objectMapper.writeValueAsString(event);

        ProducerRecord<String, String> record = buildRecord(key, value, orderEventsTopicName);

        CompletableFuture<SendResult<String, String>> future = kafkaTemplate.send(record);

        future.whenComplete((result, ex) -> {
            if (ex != null) {
                handleFailure(key, value, ex);
            } else {
                handleSuccess(key, value, result);
            }
        });

        return future;
    }

    /**
     * Build Kafka ProducerRecord with topic, key, and value.
     */
    private ProducerRecord<String, String> buildRecord(String key, String value, String topic) {
        List<Header> headers = List.of(
                new RecordHeader("event-type", "OrderEvent".getBytes(StandardCharsets.UTF_8)),
                new RecordHeader("event-source", "order-service".getBytes(StandardCharsets.UTF_8)),
                new RecordHeader("correlation-id", UUID.randomUUID().toString().getBytes(StandardCharsets.UTF_8)),
                new RecordHeader("trace-id", UUID.randomUUID().toString().getBytes(StandardCharsets.UTF_8)),
                new RecordHeader("timestamp", String.valueOf(System.currentTimeMillis()).getBytes(StandardCharsets.UTF_8)),
                new RecordHeader("content-type", "application/json".getBytes(StandardCharsets.UTF_8)),
                new RecordHeader("initiator", "system-user".getBytes(StandardCharsets.UTF_8))
        );

        return new ProducerRecord<>(topic, null, key, value, headers);
    }

    /**
     * Handle failure case when sending message to Kafka fails.
     */
    private void handleFailure(String key, String value, Throwable throwable) {
        log.error("Failed to send OrderEvent. key={}, value={}, error={}", key, value, throwable.getMessage(), throwable);
    }

    /**
     * Handle success case when Kafka acknowledges the message.
     */
    private void handleSuccess(String key, String value, SendResult<String, String> sendResult) {
        log.info("Successfully sent OrderEvent. key={}, value={}, topic={}, partition={}, offset={}",
                key,
                value,
                sendResult.getRecordMetadata().topic(),
                sendResult.getRecordMetadata().partition(),
                sendResult.getRecordMetadata().offset()
        );
    }
}
```

---

### Consumer Server

#### Environment Payment Configuration (`application.yml`)

```yml
spring:
  kafka:
    bootstrap-servers: localhost:9092, localhost:9094, localhost:9096
    consumer:
      group-id: payment-consumer-group
      auto-offset-reset: earliest
    listener:
      concurrency: 3
      ack-mode: record
      batch-listener: false
    topics:
      order-events:
        name: "example.order.events"
```

#### Kafka Consumer Configuration

```java
@Configuration
@EnableKafka
public class KafkaConfig {

    @Value("${spring.kafka.bootstrap-servers}")
    private String bootstrapServers;

    @Value("${spring.kafka.consumer.group-id:payment-consumer-group}")
    private String groupId;

    @Value("${spring.kafka.consumer.auto-offset-reset:earliest}")
    private String autoOffsetReset;

    @Value("${spring.kafka.listener.concurrency:3}")
    private Integer partitions;

    @Value("${spring.kafka.listener.ack-mode:record}")
    private ContainerProperties.AckMode ackMode;

    @Value("${spring.kafka.listener.batch-listener:false}")
    private boolean batchListener;

    @Bean
    public ConsumerFactory<String, String> consumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);

        return new DefaultKafkaConsumerFactory<>(props);
    }


    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        factory.setConcurrency(partitions); // parallelism = #partitions
        factory.getContainerProperties().setAckMode(ackMode);
        return factory;
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> batchListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        factory.setBatchListener(batchListener); // enable batch listener
        return factory;
    }
}
```

#### Kafka Message Model

```java
@Getter
@Setter
@Builder
public class OrderEvent {
    private UUID eventId;              // unique per event (important for idempotency)
    private OrderEventType eventType;  // CREATED, CANCELLED
    private OffsetDateTime eventAt;    // when the event was published
    private OrderPayload order;        // order details related to this event

    public enum OrderEventType {
        CREATED,
        PAID,
        PROCESSING,
        SHIPPED,
        DELIVERED,
        CANCELLED
    }

    @Getter
    @Builder
    @NoArgsConstructor(access = AccessLevel.PRIVATE)
    @AllArgsConstructor(access = AccessLevel.PRIVATE)
    public static class OrderPayload {
        private String orderId;
        private String customerId;
        private BigDecimal totalAmount;
        private OffsetDateTime orderAt;
    }
}
```

#### Kafka Consumer

```java
@Component
@Slf4j
public class OrderEventConsumer {

    private final ObjectMapper objectMapper;
    private final PaymentService paymentService;

    public OrderEventConsumer(ObjectMapper objectMapper, PaymentService paymentService) {
        this.objectMapper = objectMapper;
        this.paymentService = paymentService;
    }

    @KafkaListener(
            topics = "${spring.kafka.topic.order-events.name}",
            groupId = "${spring.kafka.consumer.group-id:payment-service-consumer}",
            containerFactory = "kafkaListenerContainerFactory"
    )
    public void consume(ConsumerRecord<String, String> record, Consumer<String, String> consumer) {
        try {
            processOrderEvent(record);
            TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition());
            OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(record.offset() + 1);
            consumer.commitSync(Collections.singletonMap(topicPartition, offsetAndMetadata));
        } catch (JsonProcessingException e) {
            throw new RuntimeException(e);
        }
    }

    private void processOrderEvent(ConsumerRecord<String, String> consumerRecord) throws JsonProcessingException {
        OrderEvent event = objectMapper.readValue(consumerRecord.value(), OrderEvent.class);

        switch (event.getEventType()) {
            case OrderEvent.OrderEventType.CREATED:
                PaymentDTO paymentDTO = new PaymentDTO();
                paymentDTO.setOrderId(event.getOrder().getOrderId());
                paymentDTO.setCustomerId(event.getOrder().getCustomerId());
                paymentDTO.setAmount(event.getOrder().getTotalAmount());
                paymentDTO.setStatus(Payment.Status.PENDING.name());
                paymentService.create(paymentDTO);
                break;
            default:
                log.info("Invalid Order Event Type");
        }
    }
}
```

---

### Conclusion

The `min.insync.replicas` setting plays a crucial role in defining Kafka’s durability guarantees:

1. It specifies how many replicas must acknowledge a write before it is considered successful.
2. It protects against data loss if a leader broker crashes.
3. It requires a balance between **availability** and **durability**, with `min.insync.replicas=2` being a common best practice for 3-replica clusters.

By combining `min.insync.replicas` with proper producer configurations (`acks=all`), you can build a more **resilient and fault-tolerant event-driven system**.

This forms the foundation for reliable event processing pipelines and microservice communication using Apache Kafka.
