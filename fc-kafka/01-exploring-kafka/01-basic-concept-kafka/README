# Basic Concept Kafka

Apache Kafka is a distributed event streaming platform designed to handle high-throughput, fault-tolerant, and scalable real-time data pipelines. This article outlines the fundamental concepts, operational mechanics, troubleshooting strategies, and practical examples that are essential for understanding and working with Kafka.

---

## Topic, Partition, and Segment

* **Topic**: A logical channel or category to which records are published by producers and from which consumers subscribe.
* **Partition**: Topics are split into partitions to achieve parallelism and scalability. Each partition is an ordered, immutable sequence of records.
* **Segment**: Each partition is further divided into segment files on disk. Segments help manage logs efficiently, allowing Kafka to delete or compact old data based on configured policies.

**Example: Creating a Topic**

```bash
kafka-topics.sh --create \
  --topic orders \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 2
```

---

## Broker, ZooKeeper, and KRaft

* **Broker**: A Kafka server responsible for storing data and serving client requests. A Kafka cluster typically consists of multiple brokers.
* **ZooKeeper**: Traditionally used by Kafka for metadata management, leader election, and cluster coordination.
* **KRaft (Kafka Raft)**: A newer consensus mechanism introduced to remove the dependency on ZooKeeper, offering simplified management and more robust scalability.

---

## Producer

Producers publish records to Kafka topics. They are responsible for:

* Choosing the target topic and partition.
* Configuring acknowledgment strategies to ensure delivery guarantees.
* Sending data in batches for higher throughput.

**Example: Producer (Java)**

```java
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;
import java.util.Properties;

public class OrderProducer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        for (int i = 0; i < 5; i++) {
            ProducerRecord<String, String> record =
                new ProducerRecord<>("orders", "order-" + i, "Order number " + i);
            producer.send(record);
        }

        producer.flush();
        producer.close();
    }
}
```

---

## Replication

Kafka ensures fault tolerance through replication. Each partition has a configurable replication factor, with one broker acting as the **leader** and others as **followers**. Followers replicate the leader’s data, ensuring high availability.

---

## In-Sync Replicas (ISR)

ISRs are the set of replicas that are fully caught up with the leader. Only replicas in the ISR are eligible for leader election if the current leader fails, ensuring durability and consistency.

---

## Producer Acks, Batch, Page Cache, and Flush

* **Acks**: Control durability and reliability (0, 1, or all).
* **Batching**: Producers can batch multiple records before sending, improving throughput.
* **Page Cache**: Kafka relies heavily on the OS page cache for fast disk operations.
* **Flush**: Defines when data is forcefully written to disk, balancing performance with durability.

---

## Replica Failure

If a replica falls behind or becomes unavailable, Kafka detects it via heartbeat timeouts. The replica is removed from the ISR until it catches up.

---

## Replica Recovery

Once a failed replica comes back online, it fetches missing data from the leader until it is fully synchronized and re-added to the ISR.

---

## Consumer Rebalance

Consumer groups allow multiple consumers to share work. Rebalancing occurs when consumers join, leave, or fail, redistributing partitions across the active consumers in the group.

**Example: Consumer (Java)**

```java
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.StringDeserializer;
import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class OrderConsumer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "order-service");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList("orders"));

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
            for (ConsumerRecord<String, String> record : records) {
                System.out.printf("Received message: key=%s, value=%s, partition=%d, offset=%d%n",
                        record.key(), record.value(), record.partition(), record.offset());
            }
        }
    }
}
```

---

## Partition Assignment Strategy

Strategies define how partitions are distributed:

* **Range Assignor**
* **RoundRobin Assignor**
* **Sticky Assignor**

Each has trade-offs in terms of workload distribution and stability.

---

## Cooperative Sticky Assignor

An improvement over the Sticky Assignor, this strategy minimizes partition movement during rebalances. It allows incremental cooperative rebalancing, reducing consumer disruption in large clusters.

---

## Kafka Log File

Kafka stores records in log files organized by topic and partition. Each log is an append-only structure, divided into segments for efficient indexing and retention.

---

## Log Retention and Cleanup Policy

Kafka manages log growth using:

* **Retention Policy**: Time-based (e.g., 7 days) or size-based.
* **Cleanup Policy**: Delete (remove old records) or Compact (retain only the latest record per key).

**Example: Retention Policy Configuration**

```properties
# server.properties
log.retention.hours=168      # Keep logs for 7 days
log.cleanup.policy=delete    # Old segments will be deleted
```

---

## Exactly Once Semantics (EOS)

EOS ensures that messages are neither lost nor duplicated, even in the presence of retries or failures. Kafka achieves EOS using idempotent producers and transactional APIs.

---

## Kafka Monitoring

Key metrics to monitor include:

* **Broker health** (disk usage, network throughput, CPU/memory).
* **Lag monitoring** (consumer lag relative to the producer).
* **Replication metrics** (ISR count, under-replicated partitions).

**Example: Check Consumer Group Lag**

```bash
kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --describe \
  --group order-service
```

Tools: Prometheus, Grafana, Confluent Control Center, Datadog.

---

## Troubleshooting ZooKeeper

* **Common issues**: session timeouts, connection loss, leader election stalls.
* **Resolution**: check ZooKeeper logs, validate quorum size, monitor latencies, and ensure proper heap/memory allocation.

---

## Troubleshooting Broker

* **Symptoms**: high CPU/memory usage, disk full, under-replicated partitions.
* **Resolution**: review logs, monitor network I/O, rebalance partitions, add brokers, or tune configurations like `num.io.threads` and `log.retention.bytes`.

---

## Troubleshooting Producer

* **Symptoms**: high latency, message loss, retries.
* **Resolution**: adjust `acks`, enable idempotence, tune `linger.ms` and batch size, and ensure brokers are reachable with sufficient throughput.

---

## Troubleshooting Consumer

* **Symptoms**: consumer lag, frequent rebalances, message duplication.
* **Resolution**: optimize poll intervals, increase session timeouts, choose appropriate assignment strategies, and verify commit offsets.

---

## Tuning Broker

* Optimize `num.network.threads`, `num.io.threads`, and `log.retention.ms`.
* Allocate sufficient disk throughput and memory for page cache.
* Monitor garbage collection (GC) in JVM.

---

## Tuning Producer

* Use batching (`batch.size`, `linger.ms`) for throughput.
* Enable idempotence (`enable.idempotence=true`) for reliability.
* Adjust compression (`gzip`, `snappy`, `lz4`, or `zstd`) for bandwidth savings.

---

## Tuning Consumer

* Configure `fetch.min.bytes` and `max.poll.records` for efficient pulls.
* Tune `session.timeout.ms` and `heartbeat.interval.ms` for stability.
* Balance between throughput and latency based on workload.

---

## References

1. Apache Kafka Documentation – [https://kafka.apache.org/documentation/](https://kafka.apache.org/documentation/)
2. Confluent Kafka Documentation – [https://docs.confluent.io/](https://docs.confluent.io/)
3. *Kafka: The Definitive Guide* by Neha Narkhede, Gwen Shapira, Todd Palino
4. Confluent Blog – [https://www.confluent.io/blog/](https://www.confluent.io/blog/)

---
